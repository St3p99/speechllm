data:
  data_path: "parler-tts/libritts_r_filtered"
  subset: "clean"
  split: "train.clean.100"
  amount: ":100%"
  remap_keys: {"text_normalized": "transcription"} # this will map 'text_normalized' field to 'transcription' in the dataset (parler-tts/libritts_r_filtered)
  sampling_rate: 16000
  min_duration: 0.01
  max_duration: 15.0
  num_proc_for_preprocessing: 4

training:
  # General parameters
  output_dir: ./output/
  freeze_modules: ["encoder", "text_decoder"]
  remove_unused_columns: false

  # Training parameters
  num_train_epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 1
  learning_rate: 4e-3
  warmup_steps: 1000
  bf16: true
  lr_scheduler_type: cosine
  optim: adamw_torch
  max_grad_norm: 1.0

  # Saving parameters
  save_strategy: "no"
  save_steps: 1000000000
  
  # Logging parameters
  logging_steps: 1
  logging_strategy: "steps"
  
  # DDP parameters
  ddp_find_unused_parameters: false

  report_to: none

model:
  speech_encoder_name_or_path: "microsoft/wavlm-large"
  text_decoder_name_or_path: "meta-llama/Llama-3.2-1B"
  downsample_factor: 5
  conversation_version: "llama_3_1"
  projector_n_layers: 2
  projector_activation: "relu"
  projector_hidden_size: 2048
